{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes - Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Without nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read csv created in Data_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "treino_0null = pd.read_csv(\"training_0null.csv\")\n",
    "teste_0null = pd.read_csv(\"test_0null.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "treino_0null_X = treino_0null.drop(columns=['salary']) \n",
    "treino_0null_Y = treino_0null['salary']\n",
    "teste_0null_X = teste_0null.drop(columns=['salary'])  \n",
    "teste_0null_Y = teste_0null['salary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model to predict values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb_0null = GaussianNB()\n",
    "gnb_0null.fit(treino_0null_X, treino_0null_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictNull = gnb_0null.predict(teste_0null_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check accuracy of predition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.95      0.87     11360\n",
      "           1       0.65      0.31      0.42      3700\n",
      "\n",
      "    accuracy                           0.79     15060\n",
      "   macro avg       0.73      0.63      0.64     15060\n",
      "weighted avg       0.77      0.79      0.76     15060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(teste_0null_Y,predictNull))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted      0     1    All\n",
      "Actual                       \n",
      "0          10738   622  11360\n",
      "1           2562  1138   3700\n",
      "All        13300  1760  15060\n"
     ]
    }
   ],
   "source": [
    "confusionNotNull = pd.crosstab(teste_0null_Y, predictNull, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "print(confusionNotNull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.788579\n"
     ]
    }
   ],
   "source": [
    "score = accuracy_score(teste_0null_Y,predictNull)\n",
    "print('Accuracy:{0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 15060 points : 3184\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "...       % (teste_0null_X.shape[0], (teste_0null_Y != predictNull).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling e Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = RandomOverSampler(sampling_strategy=0.4)\n",
    "treino_0null_X,treino_0null_Y = oversample.fit_resample(treino_0null_X,treino_0null_Y)\n",
    "\n",
    "undersample = RandomUnderSampler(sampling_strategy=0.7)\n",
    "treino_0null_X,treino_0null_Y = undersample.fit_resample(treino_0null_X,treino_0null_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb_0null = GaussianNB()\n",
    "gnb_0null.fit(treino_0null_X, treino_0null_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictNull = gnb_0null.predict(teste_0null_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.94      0.87     11360\n",
      "           1       0.64      0.31      0.42      3700\n",
      "\n",
      "    accuracy                           0.79     15060\n",
      "   macro avg       0.72      0.63      0.64     15060\n",
      "weighted avg       0.77      0.79      0.76     15060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(teste_0null_Y,predictNull))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted      0     1    All\n",
      "Actual                       \n",
      "0          10712   648  11360\n",
      "1           2559  1141   3700\n",
      "All        13271  1789  15060\n"
     ]
    }
   ],
   "source": [
    "confusionNotNull = pd.crosstab(teste_0null_Y, predictNull, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "print(confusionNotNull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.787052\n"
     ]
    }
   ],
   "source": [
    "score = accuracy_score(teste_0null_Y,predictNull)\n",
    "print('Accuracy:{0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 15060 points : 3207\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "...       % (teste_0null_X.shape[0], (teste_0null_Y != predictNull).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Mode as null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read csv created in Data_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "treino_mode = pd.read_csv(\"training_mode.csv\")\n",
    "teste_mode = pd.read_csv(\"test_mode.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "treino_mode_X = treino_mode.drop(columns=['salary']) \n",
    "treino_mode_Y = treino_mode['salary']\n",
    "teste_mode_X = teste_mode.drop(columns=['salary'])  \n",
    "teste_mode_Y = teste_mode['salary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model to predict values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb_mode = GaussianNB()\n",
    "gnb_mode.fit(treino_mode_X, treino_mode_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictMode = gnb_mode.predict(teste_mode_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check accuracy of predition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.95      0.87     11360\n",
      "           1       0.65      0.31      0.42      3700\n",
      "\n",
      "    accuracy                           0.79     15060\n",
      "   macro avg       0.73      0.63      0.64     15060\n",
      "weighted avg       0.77      0.79      0.76     15060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(teste_mode_Y,predictMode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted      0     1    All\n",
      "Actual                       \n",
      "0          10754   606  11360\n",
      "1           2568  1132   3700\n",
      "All        13322  1738  15060\n"
     ]
    }
   ],
   "source": [
    "confusionMode = pd.crosstab(teste_mode_Y, predictMode, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "print(confusionMode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.789243\n"
     ]
    }
   ],
   "source": [
    "score = accuracy_score(teste_mode_Y,predictMode)\n",
    "print('Accuracy:{0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 15060 points : 3174\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "...       % (teste_mode_X.shape[0], (teste_mode_Y != predictMode).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling e Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = RandomOverSampler(sampling_strategy=0.4)\n",
    "treino_mode_X,treino_mode_Y = oversample.fit_resample(treino_mode_X,treino_mode_Y)\n",
    "\n",
    "undersample = RandomUnderSampler(sampling_strategy=0.7)\n",
    "treino_mode_X,treino_mode_Y = undersample.fit_resample(treino_mode_X,treino_mode_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb_mode = GaussianNB()\n",
    "gnb_mode.fit(treino_mode_X, treino_mode_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictMode = gnb_mode.predict(teste_mode_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.94      0.87     11360\n",
      "           1       0.64      0.31      0.42      3700\n",
      "\n",
      "    accuracy                           0.79     15060\n",
      "   macro avg       0.72      0.63      0.64     15060\n",
      "weighted avg       0.77      0.79      0.76     15060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(teste_mode_Y,predictMode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted      0     1    All\n",
      "Actual                       \n",
      "0          10723   637  11360\n",
      "1           2559  1141   3700\n",
      "All        13282  1778  15060\n"
     ]
    }
   ],
   "source": [
    "confusionMode = pd.crosstab(teste_mode_Y, predictMode, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "print(confusionMode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.787782\n"
     ]
    }
   ],
   "source": [
    "score = accuracy_score(teste_mode_Y,predictMode)\n",
    "print('Accuracy:{0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 15060 points : 3196\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "...       % (teste_mode_X.shape[0], (teste_mode_Y != predictMode).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - KNN as null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read csv created in Data_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "treino_knn = pd.read_csv(\"training_knn.csv\")\n",
    "teste_knn = pd.read_csv(\"test_knn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "treino_knn_X = treino_knn.drop(columns=['salary']) \n",
    "treino_knn_Y = treino_knn['salary']\n",
    "teste_knn_X = teste_knn.drop(columns=['salary'])  \n",
    "teste_knn_Y = teste_knn['salary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model to predict values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb_knn = GaussianNB()\n",
    "gnb_knn.fit(treino_knn_X, treino_knn_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictKNN = gnb_knn.predict(teste_knn_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check accuracy of predition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.95      0.87     11360\n",
      "         1.0       0.65      0.31      0.42      3700\n",
      "\n",
      "    accuracy                           0.79     15060\n",
      "   macro avg       0.73      0.63      0.64     15060\n",
      "weighted avg       0.77      0.79      0.76     15060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(teste_knn_Y,predictKNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted    0.0   1.0    All\n",
      "Actual                       \n",
      "0.0        10755   605  11360\n",
      "1.0         2569  1131   3700\n",
      "All        13324  1736  15060\n"
     ]
    }
   ],
   "source": [
    "confusionKNN = pd.crosstab(teste_knn_Y, predictKNN, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "print(confusionKNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.789243\n"
     ]
    }
   ],
   "source": [
    "score = accuracy_score(teste_knn_Y,predictKNN)\n",
    "print('Accuracy:{0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 15060 points : 3174\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "...       % (teste_knn_X.shape[0], (teste_knn_Y != predictKNN).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling e Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = RandomOverSampler(sampling_strategy=0.4)\n",
    "treino_knn_X,treino_knn_Y = oversample.fit_resample(treino_knn_X,treino_knn_Y)\n",
    "\n",
    "undersample = RandomUnderSampler(sampling_strategy=0.7)\n",
    "treino_knn_X,treino_knn_Y = undersample.fit_resample(treino_knn_X,treino_knn_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb_knn = GaussianNB()\n",
    "gnb_knn.fit(treino_knn_X, treino_knn_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictKNN = gnb_knn.predict(teste_knn_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.94      0.87     11360\n",
      "         1.0       0.64      0.31      0.42      3700\n",
      "\n",
      "    accuracy                           0.79     15060\n",
      "   macro avg       0.72      0.63      0.64     15060\n",
      "weighted avg       0.77      0.79      0.76     15060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(teste_knn_Y,predictKNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted    0.0   1.0    All\n",
      "Actual                       \n",
      "0.0        10707   653  11360\n",
      "1.0         2559  1141   3700\n",
      "All        13266  1794  15060\n"
     ]
    }
   ],
   "source": [
    "confusionKNN = pd.crosstab(teste_knn_Y, predictKNN, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "print(confusionKNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.786720\n"
     ]
    }
   ],
   "source": [
    "score = accuracy_score(teste_knn_Y,predictKNN)\n",
    "print('Accuracy:{0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 15060 points : 3212\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "...       % (teste_knn_X.shape[0], (teste_knn_Y != predictKNN).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - No null but using scale instead of ints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read csv created in Data_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "treino_strings = pd.read_csv(\"training_0null_strings.csv\")\n",
    "teste_strings = pd.read_csv(\"test_0null_strings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "treino_strings_X = treino_strings.drop(columns=['salary']) \n",
    "treino_strings_Y = treino_strings['salary']\n",
    "teste_strings_X = teste_strings.drop(columns=['salary'])  \n",
    "teste_strings_Y = teste_strings['salary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "categorical = ['workclass', 'maritalstatus', 'occupation', 'relationship', 'race', 'sex', 'nativecountry']\n",
    "for feature in categorical:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        treino_strings_X[feature] = le.fit_transform(treino_strings_X[feature])\n",
    "        teste_strings_X[feature] = le.transform(teste_strings_X[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "treino_strings_X = pd.DataFrame(scaler.fit_transform(treino_strings_X), columns = treino_strings_X.columns)\n",
    "\n",
    "teste_strings_X = pd.DataFrame(scaler.transform(teste_strings_X), columns = teste_strings_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model to predict values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb_strings = GaussianNB()\n",
    "gnb_strings.fit(treino_strings_X, treino_strings_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictStrings = gnb_strings.predict(teste_strings_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check accuracy of predition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.95      0.88     11360\n",
      "           1       0.68      0.32      0.44      3700\n",
      "\n",
      "    accuracy                           0.80     15060\n",
      "   macro avg       0.74      0.64      0.66     15060\n",
      "weighted avg       0.78      0.80      0.77     15060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(teste_strings_Y,predictStrings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted      0     1    All\n",
      "Actual                       \n",
      "0          10787   573  11360\n",
      "1           2500  1200   3700\n",
      "All        13287  1773  15060\n"
     ]
    }
   ],
   "source": [
    "confusionStrings = pd.crosstab(teste_strings_Y, predictStrings, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "print(confusionStrings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.795950\n"
     ]
    }
   ],
   "source": [
    "score = accuracy_score(teste_strings_Y,predictStrings)\n",
    "print('Accuracy:{0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 15060 points : 3073\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "...       % (teste_strings_X.shape[0], (teste_strings_Y != predictStrings).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling e Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = RandomOverSampler(sampling_strategy=0.4)\n",
    "treino_strings_X,treino_strings_Y = oversample.fit_resample(treino_strings_X,treino_strings_Y)\n",
    "\n",
    "undersample = RandomUnderSampler(sampling_strategy=0.7)\n",
    "treino_strings_X,treino_strings_Y = undersample.fit_resample(treino_strings_X,treino_strings_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb_strings = GaussianNB()\n",
    "gnb_strings.fit(treino_strings_X, treino_strings_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictStrings = gnb_strings.predict(teste_strings_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.94      0.88     11360\n",
      "           1       0.68      0.39      0.50      3700\n",
      "\n",
      "    accuracy                           0.81     15060\n",
      "   macro avg       0.75      0.67      0.69     15060\n",
      "weighted avg       0.79      0.81      0.79     15060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(teste_strings_Y,predictStrings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted      0     1    All\n",
      "Actual                       \n",
      "0          10683   677  11360\n",
      "1           2246  1454   3700\n",
      "All        12929  2131  15060\n"
     ]
    }
   ],
   "source": [
    "confusionStrings = pd.crosstab(teste_strings_Y, predictStrings, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "print(confusionStrings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.805910\n"
     ]
    }
   ],
   "source": [
    "score = accuracy_score(teste_strings_Y,predictStrings)\n",
    "print('Accuracy:{0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 15060 points : 2923\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "...       % (teste_strings_X.shape[0], (teste_strings_Y != predictStrings).sum()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
